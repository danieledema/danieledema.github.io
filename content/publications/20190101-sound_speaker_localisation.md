+++
date = '2019-11-20T12:00:00Z'
draft = false
title = 'No Need to Scream: Robust Sound-based Speaker Localisation in Challenging Scenarios'
tags = ["conference", "speaker_localisation", "sound"]
+++

### Abstract
This paper is about speaker verification and horizontal localisation in the presence of conspicuous noise. Specifically, we are interested in enabling a mobile robot to robustly and accurately spot the presence of a target speaker and estimate his/her position in challenging acoustic scenarios. While several solutions to both tasks have been proposed in the literature, little attention has been devoted to the development of systems able to function in harsh noisy conditions. To address these shortcomings, in this work we follow a purely data-driven approach based on deep learning architectures which, by not requiring any knowledge either on the nature of the masking noise or on the structure and acoustics of the operation environment, it is able to reliably act in previously unexplored acoustic scenes. Our experimental evaluation, relying on data collected in real environments with a robotic platform, demonstrates that our framework is able to achieve high performance both in the verification and localisation tasks, despite the presence of copious noise.

### Useful links
Paper: [SpringerLink](https://link.springer.com/chapter/10.1007/978-3-030-35888-4_17)

### Bibtex
```bibtex
@InProceedings{10.1007/978-3-030-35888-4_17,
author="Tse, Tze Ho Elden
and De Martini, Daniele
and Marchegiani, Letizia",
editor="Salichs, Miguel A.
and Ge, Shuzhi Sam
and Barakova, Emilia Ivanova
and Cabibihan, John-John
and Wagner, Alan R.
and Castro-Gonz{\'a}lez, {\'A}lvaro
and He, Hongsheng",
title="No Need to Scream: Robust Sound-Based Speaker Localisation in Challenging Scenarios",
booktitle="Social Robotics",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="176--185",
abstract="This paper is about speaker verification and horizontal localisation in the presence of conspicuous noise. Specifically, we are interested in enabling a mobile robot to robustly and accurately spot the presence of a target speaker and estimate his/her position in challenging acoustic scenarios. While several solutions to both tasks have been proposed in the literature, little attention has been devoted to the development of systems able to function in harsh noisy conditions. To address these shortcomings, in this work we follow a purely data-driven approach based on deep learning architectures which, by not requiring any knowledge either on the nature of the masking noise or on the structure and acoustics of the operation environment, it is able to reliably act in previously unexplored acoustic scenes. Our experimental evaluation, relying on data collected in real environments with a robotic platform, demonstrates that our framework is able to achieve high performance both in the verification and localisation tasks, despite the presence of copious noise.",
isbn="978-3-030-35888-4"
}
```
